{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-aqEt1BXySm"
      },
      "source": [
        "# Neural Networks for Binary Classification\\n",
        "\\n",
        "**Multi-Layer Perceptron Implementation from Scratch**\\n",
        "\\n",
        "This notebook demonstrates the implementation and comparison of MLP classifiers:\\n",
        "- Custom NumPy-based MLP with SGD optimization\\n",
        "- Adam optimizer implementation with early stopping\\n",
        "- Comparison with scikit-learn and PyTorch implementations\\n",
        "\\n",
        "Dataset: Vehicle Purchase Quality Prediction (\\\"Don't Get Kicked!\\\" from Kaggle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mzOWVWFFXySq"
      },
      "outputs": [],
      "source": [
        "!pip install -q pandas numpy matplotlib scikit-learn torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Qpvo1cXySs"
      },
      "source": [
        "---\n",
        "## 1. Data Preparation and Temporal Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk8UesWlXySt",
        "outputId": "23a4556c-982c-4a80-8ec6-7fd59adc3cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cu126\n",
            "NumPy version: 2.0.2\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Импорт всех необходимых библиотек\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Фиксация random seed для воспроизводимости\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'NumPy version: {np.__version__}')\n",
        "print(f'CUDA available: {torch.cuda.is_available()}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voeFFmz-XySu",
        "outputId": "2bd487d6-2d6b-43a1-a9b3-66d8d8cff929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер датасета: (72983, 33)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 72983 entries, 1 to 73014\n",
            "Data columns (total 33 columns):\n",
            " #   Column                             Non-Null Count  Dtype         \n",
            "---  ------                             --------------  -----         \n",
            " 0   IsBadBuy                           72983 non-null  int64         \n",
            " 1   PurchDate                          72983 non-null  datetime64[ns]\n",
            " 2   Auction                            72983 non-null  object        \n",
            " 3   VehYear                            72983 non-null  int64         \n",
            " 4   VehicleAge                         72983 non-null  int64         \n",
            " 5   Make                               72983 non-null  object        \n",
            " 6   Model                              72983 non-null  object        \n",
            " 7   Trim                               70623 non-null  object        \n",
            " 8   SubModel                           72975 non-null  object        \n",
            " 9   Color                              72975 non-null  object        \n",
            " 10  Transmission                       72974 non-null  object        \n",
            " 11  WheelTypeID                        69814 non-null  float64       \n",
            " 12  WheelType                          69809 non-null  object        \n",
            " 13  VehOdo                             72983 non-null  int64         \n",
            " 14  Nationality                        72978 non-null  object        \n",
            " 15  Size                               72978 non-null  object        \n",
            " 16  TopThreeAmericanName               72978 non-null  object        \n",
            " 17  MMRAcquisitionAuctionAveragePrice  72965 non-null  float64       \n",
            " 18  MMRAcquisitionAuctionCleanPrice    72965 non-null  float64       \n",
            " 19  MMRAcquisitionRetailAveragePrice   72965 non-null  float64       \n",
            " 20  MMRAcquisitonRetailCleanPrice      72965 non-null  float64       \n",
            " 21  MMRCurrentAuctionAveragePrice      72668 non-null  float64       \n",
            " 22  MMRCurrentAuctionCleanPrice        72668 non-null  float64       \n",
            " 23  MMRCurrentRetailAveragePrice       72668 non-null  float64       \n",
            " 24  MMRCurrentRetailCleanPrice         72668 non-null  float64       \n",
            " 25  PRIMEUNIT                          3419 non-null   object        \n",
            " 26  AUCGUART                           3419 non-null   object        \n",
            " 27  BYRNO                              72983 non-null  int64         \n",
            " 28  VNZIP1                             72983 non-null  int64         \n",
            " 29  VNST                               72983 non-null  object        \n",
            " 30  VehBCost                           72983 non-null  float64       \n",
            " 31  IsOnlineSale                       72983 non-null  int64         \n",
            " 32  WarrantyCost                       72983 non-null  int64         \n",
            "dtypes: datetime64[ns](1), float64(10), int64(8), object(14)\n",
            "memory usage: 18.9+ MB\n"
          ]
        }
      ],
      "source": [
        "# Загрузка данных\n",
        "data = pd.read_csv('data/training.csv', index_col=0, parse_dates=['PurchDate'])\n",
        "print(f'Размер датасета: {data.shape}')\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvDoYXUeXySv",
        "outputId": "c97efbce-7103-4b94-a64d-585f23c46596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Числовые признаки (17): ['VehYear', 'VehicleAge', 'WheelTypeID', 'VehOdo', 'MMRAcquisitionAuctionAveragePrice', 'MMRAcquisitionAuctionCleanPrice', 'MMRAcquisitionRetailAveragePrice', 'MMRAcquisitonRetailCleanPrice', 'MMRCurrentAuctionAveragePrice', 'MMRCurrentAuctionCleanPrice', 'MMRCurrentRetailAveragePrice', 'MMRCurrentRetailCleanPrice', 'BYRNO', 'VNZIP1', 'VehBCost', 'IsOnlineSale', 'WarrantyCost']\n",
            "\n",
            "Категориальные признаки (14): ['Auction', 'Make', 'Model', 'Trim', 'SubModel', 'Color', 'Transmission', 'WheelType', 'Nationality', 'Size', 'TopThreeAmericanName', 'PRIMEUNIT', 'AUCGUART', 'VNST']\n"
          ]
        }
      ],
      "source": [
        "# Разделение признаков на категориальные и числовые\n",
        "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_cols = data.select_dtypes(include=['number']).columns.tolist()\n",
        "numerical_cols.remove('IsBadBuy')  # Убираем целевую переменную\n",
        "\n",
        "print(f'Числовые признаки ({len(numerical_cols)}): {numerical_cols}')\n",
        "print(f'\\nКатегориальные признаки ({len(categorical_cols)}): {categorical_cols}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooRSkGV8XySw",
        "outputId": "8be9f1cc-c855-46bf-905e-3adb0efe7d27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Разбиение датасета (train/valid/test):\n",
            "  Train: 33.3% (24327 samples)\n",
            "  Valid: 33.3% (24328 samples)\n",
            "  Test:  33.3% (24328 samples)\n",
            "\n",
            "Диапазоны дат:\n",
            "  Train: 2009-01-05 00:00:00 — 2009-09-15 00:00:00\n",
            "  Valid: 2009-09-15 00:00:00 — 2010-05-14 00:00:00\n",
            "  Test:  2010-05-14 00:00:00 — 2010-12-30 00:00:00\n"
          ]
        }
      ],
      "source": [
        "def temporal_split(df, train_ratio=1/3, valid_ratio=1/3):\n",
        "    \"\"\"\n",
        "    Разбиение датасета по времени:\n",
        "    train.PurchDate < valid.PurchDate < test.PurchDate\n",
        "\n",
        "    Первые 33% записей (по дате) — train, средние 33% — valid, последние 33% — test.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df['PurchDate'] = pd.to_datetime(df['PurchDate'])\n",
        "    df = df.sort_values(by='PurchDate').reset_index(drop=True)\n",
        "\n",
        "    n = len(df)\n",
        "    split1_idx = int(n * train_ratio)\n",
        "    split2_idx = int(n * (train_ratio + valid_ratio))\n",
        "\n",
        "    df_train = df.iloc[:split1_idx].reset_index(drop=True)\n",
        "    df_valid = df.iloc[split1_idx:split2_idx].reset_index(drop=True)\n",
        "    df_test = df.iloc[split2_idx:].reset_index(drop=True)\n",
        "\n",
        "    print('Разбиение датасета (train/valid/test):')\n",
        "    print(f'  Train: {len(df_train)/len(df)*100:.1f}% ({len(df_train)} samples)')\n",
        "    print(f'  Valid: {len(df_valid)/len(df)*100:.1f}% ({len(df_valid)} samples)')\n",
        "    print(f'  Test:  {len(df_test)/len(df)*100:.1f}% ({len(df_test)} samples)')\n",
        "    print(f'\\nДиапазоны дат:')\n",
        "    print(f'  Train: {df_train[\"PurchDate\"].min()} — {df_train[\"PurchDate\"].max()}')\n",
        "    print(f'  Valid: {df_valid[\"PurchDate\"].min()} — {df_valid[\"PurchDate\"].max()}')\n",
        "    print(f'  Test:  {df_test[\"PurchDate\"].min()} — {df_test[\"PurchDate\"].max()}')\n",
        "\n",
        "    return df_train, df_valid, df_test\n",
        "\n",
        "\n",
        "# Разбиение данных по времени\n",
        "df_train, df_valid, df_test = temporal_split(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOl2GhJQXySx",
        "outputId": "0690ab2a-ed17-4014-cecc-7f3b9a3333a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность после предобработки:\n",
            "  X_train: (24327, 1716)\n",
            "  X_valid: (24328, 1716)\n",
            "  X_test:  (24328, 1716)\n"
          ]
        }
      ],
      "source": [
        "def prepare_features(df_train, df_valid, df_test, cat_cols, num_cols):\n",
        "    \"\"\"\n",
        "    Подготовка признаков: заполнение пропусков, кодирование категориальных переменных,\n",
        "    масштабирование числовых признаков.\n",
        "    \"\"\"\n",
        "    # Извлекаем целевую переменную\n",
        "    y_train = df_train['IsBadBuy'].values\n",
        "    y_valid = df_valid['IsBadBuy'].values\n",
        "    y_test = df_test['IsBadBuy'].values\n",
        "\n",
        "    # Убираем целевую переменную и дату из признаков\n",
        "    X_train = df_train.drop(columns=['IsBadBuy', 'PurchDate'])\n",
        "    X_valid = df_valid.drop(columns=['IsBadBuy', 'PurchDate'])\n",
        "    X_test = df_test.drop(columns=['IsBadBuy', 'PurchDate'])\n",
        "\n",
        "    # Заполнение пропусков: категориальные — 'missing'\n",
        "    for col in cat_cols:\n",
        "        X_train[col] = X_train[col].fillna('missing').astype(str)\n",
        "        X_valid[col] = X_valid[col].fillna('missing').astype(str)\n",
        "        X_test[col] = X_test[col].fillna('missing').astype(str)\n",
        "\n",
        "    # Числовые: заполнение медианой из train\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
        "    X_valid[num_cols] = num_imputer.transform(X_valid[num_cols])\n",
        "    X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
        "\n",
        "    # One-Hot Encoding для категориальных признаков\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    X_train_cat = encoder.fit_transform(X_train[cat_cols])\n",
        "    X_valid_cat = encoder.transform(X_valid[cat_cols])\n",
        "    X_test_cat = encoder.transform(X_test[cat_cols])\n",
        "\n",
        "    # Масштабирование числовых признаков\n",
        "    scaler = StandardScaler()\n",
        "    X_train_num = scaler.fit_transform(X_train[num_cols])\n",
        "    X_valid_num = scaler.transform(X_valid[num_cols])\n",
        "    X_test_num = scaler.transform(X_test[num_cols])\n",
        "\n",
        "    # Объединение признаков: числовые + категориальные\n",
        "    X_train_processed = np.hstack([X_train_num, X_train_cat])\n",
        "    X_valid_processed = np.hstack([X_valid_num, X_valid_cat])\n",
        "    X_test_processed = np.hstack([X_test_num, X_test_cat])\n",
        "\n",
        "    print(f'Размерность после предобработки:')\n",
        "    print(f'  X_train: {X_train_processed.shape}')\n",
        "    print(f'  X_valid: {X_valid_processed.shape}')\n",
        "    print(f'  X_test:  {X_test_processed.shape}')\n",
        "\n",
        "    return X_train_processed, X_valid_processed, X_test_processed, y_train, y_valid, y_test\n",
        "\n",
        "\n",
        "# Подготовка признаков\n",
        "X_train, X_valid, X_test, y_train, y_valid, y_test = prepare_features(\n",
        "    df_train, df_valid, df_test, categorical_cols, numerical_cols\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R46NS8eqXySy",
        "outputId": "0cf11bf9-1431-4893-985c-14fed7737499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Распределение IsBadBuy:\n",
            "  Train: 0.1149 (2794 / 24327)\n",
            "  Valid: 0.1301 (3164 / 24328)\n",
            "  Test:  0.1241 (3018 / 24328)\n"
          ]
        }
      ],
      "source": [
        "# Проверка распределения целевой переменной\n",
        "print('Распределение IsBadBuy:')\n",
        "print(f'  Train: {y_train.mean():.4f} ({y_train.sum()} / {len(y_train)})')\n",
        "print(f'  Valid: {y_valid.mean():.4f} ({y_valid.sum()} / {len(y_valid)})')\n",
        "print(f'  Test:  {y_test.mean():.4f} ({y_test.sum()} / {len(y_test)})')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rFFUKj9BXySz"
      },
      "outputs": [],
      "source": [
        "def compute_gini(y_true, y_prob):\n",
        "    \"\"\"Вычисление коэффициента Gini: Gini = 2 * AUC - 1\"\"\"\n",
        "    auc = roc_auc_score(y_true, y_prob)\n",
        "    return 2 * auc - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZAhYLLnXySz"
      },
      "source": [
        "---\n",
        "## 2. MLP Implementation from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3L62TiUyXyS0"
      },
      "outputs": [],
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\"\n",
        "    Класс с реализациями функций активации и их производных.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        \"\"\"Сигмоидная функция: σ(x) = 1 / (1 + exp(-x))\"\"\"\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(x):\n",
        "        \"\"\"Производная sigmoid: σ'(x) = σ(x) * (1 - σ(x))\"\"\"\n",
        "        s = ActivationFunctions.sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        \"\"\"ReLU: max(0, x)\"\"\"\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(x):\n",
        "        \"\"\"Производная ReLU: 1 если x > 0, иначе 0\"\"\"\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine(x):\n",
        "        \"\"\"Косинусная активация\"\"\"\n",
        "        return np.cos(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def cosine_derivative(x):\n",
        "        \"\"\"Производная cosine: -sin(x)\"\"\"\n",
        "        return -np.sin(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "M7xPuPbnXyS1"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Многослойный перцептрон с 1 скрытым слоем.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=100, activation='sigmoid', learning_rate=0.01,\n",
        "                 n_epochs=100, batch_size=32, random_state=42, verbose=True):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_epochs = n_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Устанавливаем функцию активации\n",
        "        self._set_activation(activation)\n",
        "\n",
        "        # Веса инициализируются при вызове fit()\n",
        "        self.W1 = None\n",
        "        self.b1 = None\n",
        "        self.W2 = None\n",
        "        self.b2 = None\n",
        "\n",
        "        # История потерь для визуализации\n",
        "        self.train_losses = []\n",
        "        self.valid_losses = []\n",
        "\n",
        "    def _set_activation(self, activation):\n",
        "        \"\"\"\n",
        "        Установка функции активации и её производной.\n",
        "        \"\"\"\n",
        "        if callable(activation):\n",
        "            self.activation = activation\n",
        "            self.activation_derivative = None\n",
        "            self.activation_name = getattr(activation, '__name__', 'custom')\n",
        "        elif activation == 'sigmoid':\n",
        "            self.activation = ActivationFunctions.sigmoid\n",
        "            self.activation_derivative = ActivationFunctions.sigmoid_derivative\n",
        "            self.activation_name = 'sigmoid'\n",
        "        elif activation == 'relu':\n",
        "            self.activation = ActivationFunctions.relu\n",
        "            self.activation_derivative = ActivationFunctions.relu_derivative\n",
        "            self.activation_name = 'relu'\n",
        "        elif activation == 'cosine':\n",
        "            self.activation = ActivationFunctions.cosine\n",
        "            self.activation_derivative = ActivationFunctions.cosine_derivative\n",
        "            self.activation_name = 'cosine'\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation: {activation}\")\n",
        "\n",
        "    def _numerical_derivative(self, x, eps=1e-7):\n",
        "        \"\"\"Численное приближение производной (центральная разность).\"\"\"\n",
        "        return (self.activation(x + eps) - self.activation(x - eps)) / (2 * eps)\n",
        "\n",
        "    def _initialize_weights(self, n_features):\n",
        "        \"\"\"\n",
        "        Инициализация весов методом Xavier/Glorot.\n",
        "        limit = sqrt(6 / (fan_in + fan_out))\n",
        "        \"\"\"\n",
        "        np.random.seed(self.random_state)\n",
        "\n",
        "        # Первый слой: n_features → n_hidden\n",
        "        limit1 = np.sqrt(6.0 / (n_features + self.n_hidden))\n",
        "        self.W1 = np.random.uniform(-limit1, limit1, (n_features, self.n_hidden))\n",
        "        self.b1 = np.zeros((1, self.n_hidden))\n",
        "\n",
        "        # Второй слой: n_hidden → 1\n",
        "        limit2 = np.sqrt(6.0 / (self.n_hidden + 1))\n",
        "        self.W2 = np.random.uniform(-limit2, limit2, (self.n_hidden, 1))\n",
        "        self.b2 = np.zeros((1, 1))\n",
        "\n",
        "    def _forward(self, X):\n",
        "        \"\"\"Прямой проход через сеть.\"\"\"\n",
        "        # Скрытый слой\n",
        "        z1 = X @ self.W1 + self.b1           # Линейная комбинация\n",
        "        a1 = self.activation(z1)              # Активация\n",
        "\n",
        "        # Выходной слой\n",
        "        z2 = a1 @ self.W2 + self.b2          # Линейная комбинация\n",
        "        a2 = ActivationFunctions.sigmoid(z2)  # Sigmoid для вероятности\n",
        "\n",
        "        # Сохраняем промежуточные значения для backpropagation\n",
        "        cache = {'X': X, 'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2}\n",
        "        return cache\n",
        "\n",
        "    def _compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Binary Cross Entropy Loss.\"\"\"\n",
        "        if y_pred.ndim == 2:\n",
        "            y_pred = y_pred[:, 1] if y_pred.shape[1] == 2 else y_pred.flatten()\n",
        "\n",
        "        eps = 1e-15  # Защита от log(0)\n",
        "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return loss\n",
        "\n",
        "    def _backward(self, y_true, cache):\n",
        "        \"\"\"Обратное распространение ошибки (backpropagation).\"\"\"\n",
        "        m = y_true.shape[0]  # Размер батча\n",
        "        y_true = y_true.reshape(-1, 1)\n",
        "\n",
        "        # Извлекаем значения из cache\n",
        "        X = cache['X']\n",
        "        z1 = cache['z1']\n",
        "        a1 = cache['a1']\n",
        "        a2 = cache['a2']\n",
        "\n",
        "        # Градиент для выходного слоя\n",
        "        dz2 = a2 - y_true                              # Ошибка выхода\n",
        "        dW2 = (1/m) * (a1.T @ dz2)                     # Градиент весов W2\n",
        "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)  # Градиент смещения b2\n",
        "\n",
        "        # Градиент для скрытого слоя (chain rule)\n",
        "        da1 = dz2 @ self.W2.T                          # Ошибка, пришедшая в скрытый слой\n",
        "        if self.activation_derivative is not None:\n",
        "            dz1 = da1 * self.activation_derivative(z1)\n",
        "        else:\n",
        "            dz1 = da1 * self._numerical_derivative(z1)\n",
        "\n",
        "        dW1 = (1/m) * (X.T @ dz1)                      # Градиент весов W1\n",
        "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)  # Градиент смещения b1\n",
        "\n",
        "        return {'dW1': dW1, 'db1': db1, 'dW2': dW2, 'db2': db2}\n",
        "\n",
        "    def _update_weights_sgd(self, gradients):\n",
        "        \"\"\"Обновление весов методом SGD: w = w - lr * gradient\"\"\"\n",
        "        self.W1 -= self.learning_rate * gradients['dW1']\n",
        "        self.b1 -= self.learning_rate * gradients['db1']\n",
        "        self.W2 -= self.learning_rate * gradients['dW2']\n",
        "        self.b2 -= self.learning_rate * gradients['db2']\n",
        "\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None):\n",
        "        \"\"\"Обучение модели.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_weights(n_features)\n",
        "        n_batches = int(np.ceil(n_samples / self.batch_size))\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # Перемешиваем данные в начале каждой эпохи\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            # Проходим по батчам\n",
        "            for i in range(n_batches):\n",
        "                start_idx = i * self.batch_size\n",
        "                end_idx = min((i + 1) * self.batch_size, n_samples)\n",
        "\n",
        "                X_batch = X_shuffled[start_idx:end_idx]\n",
        "                y_batch = y_shuffled[start_idx:end_idx]\n",
        "\n",
        "                # Forward → Loss → Backward → Update\n",
        "                cache = self._forward(X_batch)\n",
        "                batch_loss = self._compute_loss(y_batch, cache['a2'])\n",
        "                epoch_loss += batch_loss * len(y_batch)\n",
        "\n",
        "                gradients = self._backward(y_batch, cache)\n",
        "                self._update_weights_sgd(gradients)\n",
        "\n",
        "            epoch_loss /= n_samples\n",
        "            self.train_losses.append(epoch_loss)\n",
        "\n",
        "            # Валидация\n",
        "            if X_valid is not None:\n",
        "                valid_proba = self.predict_proba(X_valid)[:, 1]\n",
        "                valid_loss = self._compute_loss(y_valid, valid_proba)\n",
        "                self.valid_losses.append(valid_loss)\n",
        "\n",
        "            # Вывод прогресса\n",
        "            if self.verbose and (epoch + 1) % 10 == 0:\n",
        "                msg = f\"Epoch {epoch+1}/{self.n_epochs} - Train Loss: {epoch_loss:.4f}\"\n",
        "                if X_valid is not None:\n",
        "                    msg += f\" - Valid Loss: {valid_loss:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Предсказание вероятностей.\"\"\"\n",
        "        cache = self._forward(X)\n",
        "        prob_1 = cache['a2'].flatten()\n",
        "        prob_0 = 1 - prob_1\n",
        "        return np.column_stack([prob_0, prob_1])\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"Предсказание классов.\"\"\"\n",
        "        proba = self.predict_proba(X)[:, 1]\n",
        "        return (proba >= threshold).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9E3wBXXyS4"
      },
      "source": [
        "---\n",
        "## 3. Achieving Gini >= 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrsjhuwiXyS4",
        "outputId": "92d7d26f-6ddf-470c-9360-028701b10813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100 - Train Loss: 0.3258 - Valid Loss: 0.3629\n",
            "Epoch 20/100 - Train Loss: 0.3100 - Valid Loss: 0.3483\n",
            "Epoch 30/100 - Train Loss: 0.2994 - Valid Loss: 0.3373\n",
            "Epoch 40/100 - Train Loss: 0.2954 - Valid Loss: 0.3372\n",
            "Epoch 50/100 - Train Loss: 0.2939 - Valid Loss: 0.3342\n",
            "Epoch 60/100 - Train Loss: 0.2930 - Valid Loss: 0.3349\n",
            "Epoch 70/100 - Train Loss: 0.2924 - Valid Loss: 0.3371\n",
            "Epoch 80/100 - Train Loss: 0.2917 - Valid Loss: 0.3323\n",
            "Epoch 90/100 - Train Loss: 0.2913 - Valid Loss: 0.3343\n",
            "Epoch 100/100 - Train Loss: 0.2907 - Valid Loss: 0.3409\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MLP at 0x7d74feecbb00>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Создание и обучение модели\n",
        "model = MLP(\n",
        "    n_hidden=100,\n",
        "    activation='sigmoid',\n",
        "    learning_rate=0.01,\n",
        "    n_epochs=100,\n",
        "    batch_size=32,\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYpQDt3JXyS4",
        "outputId": "7840e476-c803-4032-b673-b6461791ac52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты на валидации:\n",
            "  Gini coefficient: 0.4906\n",
            "  ROC AUC: 0.7453\n",
            "\n",
            "Отчёт классификации:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.98      0.94     21164\n",
            "           1       0.64      0.21      0.31      3164\n",
            "\n",
            "    accuracy                           0.88     24328\n",
            "   macro avg       0.77      0.59      0.62     24328\n",
            "weighted avg       0.86      0.88      0.85     24328\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Оценка на валидации\n",
        "y_valid_proba = model.predict_proba(X_valid)[:, 1]\n",
        "y_valid_pred = model.predict(X_valid)\n",
        "\n",
        "gini_valid = compute_gini(y_valid, y_valid_proba)\n",
        "print(f'Результаты на валидации:')\n",
        "print(f'  Gini coefficient: {gini_valid:.4f}')\n",
        "print(f'  ROC AUC: {roc_auc_score(y_valid, y_valid_proba):.4f}')\n",
        "print(f'\\nОтчёт классификации:')\n",
        "print(classification_report(y_valid, y_valid_pred, zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScQnqwm6XyS4"
      },
      "source": [
        "---\n",
        "## 4. Comparison with sklearn MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZKw10OjXyS4",
        "outputId": "02724b55-a445-466d-cb5c-6e2ee95c973c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn MLPClassifier (SGD):\n",
            "  Gini coefficient: 0.4575\n",
            "  ROC AUC: 0.7288\n"
          ]
        }
      ],
      "source": [
        "# sklearn MLPClassifier с SGD\n",
        "sklearn_mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    activation='logistic',\n",
        "    solver='sgd',\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=100,\n",
        "    batch_size=32,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "sklearn_mlp.fit(X_train, y_train)\n",
        "\n",
        "sklearn_proba = sklearn_mlp.predict_proba(X_valid)[:, 1]\n",
        "sklearn_pred = sklearn_mlp.predict(X_valid)\n",
        "\n",
        "sklearn_gini = compute_gini(y_valid, sklearn_proba)\n",
        "print('sklearn MLPClassifier (SGD):')\n",
        "print(f'  Gini coefficient: {sklearn_gini:.4f}')\n",
        "print(f'  ROC AUC: {roc_auc_score(y_valid, sklearn_proba):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm0AwJFGXyS5",
        "outputId": "6daaac0f-260f-4201-8bc2-60a55184b559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сравнение моделей:\n",
            "Модель                          Gini\n",
            "-------------------------------------\n",
            "My MLP (sigmoid)              0.4906\n",
            "sklearn MLP (SGD)             0.4575\n"
          ]
        }
      ],
      "source": [
        "# Сравнительная таблица\n",
        "print('Сравнение моделей:')\n",
        "print(f'{\"Модель\":<25} {\"Gini\":>10}')\n",
        "print('-' * 37)\n",
        "print(f'{\"My MLP (sigmoid)\":<25} {gini_valid:>10.4f}')\n",
        "print(f'{\"sklearn MLP (SGD)\":<25} {sklearn_gini:>10.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучше ли sklearn чем моя реализация?\n",
        "\n",
        "Моя реализация лучше на 0.0331 Gini.\n",
        "\n",
        "Возможные причины:\n",
        "\n",
        "- Xavier/Glorot инициализация хорошо подходит для sigmoid\n",
        "- Мой MLP — \"чистый\", а sklearn по умолчанию добавляет momentum=0.9 и L2-регуляризацию (alpha=0.0001), что меняет поведение алгоритма"
      ],
      "metadata": {
        "id": "pTTwCJZoPWvd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx4e69GZXyS5"
      },
      "source": [
        "---\n",
        "## 5. Activation Function Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtvRhjlWXyS5",
        "outputId": "43619760-b679-4491-c64c-bc6cae002cde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сравнение функций активации (sigmoid, ReLU, cosine)\n",
            "\n",
            "Параметры: n_hidden=100, lr=0.01, epochs=100\n",
            "\n",
            "   SIGMOID: Gini = 0.4906\n",
            "      RELU: Gini = 0.4712\n",
            "    COSINE: Gini = 0.4884\n",
            "\n",
            "Лучшая функция активации: sigmoid\n"
          ]
        }
      ],
      "source": [
        "fixed_params = {\n",
        "    'n_hidden': 100,\n",
        "    'learning_rate': 0.01,\n",
        "    'n_epochs': 100,\n",
        "    'batch_size': 32,\n",
        "    'random_state': 42,\n",
        "    'verbose': False\n",
        "}\n",
        "\n",
        "activations = ['sigmoid', 'relu', 'cosine']\n",
        "activation_results = {}\n",
        "\n",
        "print(f'Сравнение функций активации (sigmoid, ReLU, cosine)\\n')\n",
        "print(f'Параметры: n_hidden={fixed_params[\"n_hidden\"]}, '\n",
        "      f'lr={fixed_params[\"learning_rate\"]}, epochs={fixed_params[\"n_epochs\"]}\\n')\n",
        "\n",
        "for act in activations:\n",
        "    mlp = MLP(activation=act, **fixed_params)\n",
        "    mlp.fit(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    y_proba = mlp.predict_proba(X_valid)[:, 1]\n",
        "    gini = compute_gini(y_valid, y_proba)\n",
        "\n",
        "    activation_results[act] = {'gini': gini, 'model': mlp}\n",
        "    print(f'{act.upper():>10}: Gini = {gini:.4f}')\n",
        "\n",
        "best_activation = max(activation_results, key=lambda x: activation_results[x]['gini'])\n",
        "print(f'\\nЛучшая функция активации: {best_activation}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Какая функция активации лучшая?\n",
        "\n",
        "Ответ: Sigmoid с Gini = 0.4906\n",
        "\n",
        "Рейтинг:\n",
        "\n",
        "- sigmoid: 0.4906\n",
        "- cosine: 0.4884\n",
        "- relu: 0.4712\n",
        "\n",
        "Объяснение:\n",
        "\n",
        "Sigmoid показывает лучший результат, вероятно потому что:\n",
        "\n",
        "- Выход всегда от 0 до 1 — это сразу можно читать как вероятность \"плохой покупки\"\n",
        "- Sigmoid даёт плавные (не резкие) градиенты, что помогает при большом количестве признаков (у нас 1716)\n",
        "- Xavier инициализация изначально разрабатывалась именно для sigmoid, поэтому они хорошо работают вместе"
      ],
      "metadata": {
        "id": "y6gDTDq-Pxpj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kg6YQoeXyS5"
      },
      "source": [
        "---\n",
        "## 6. PyTorch MLP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmGWYAiGXyS6",
        "outputId": "25cf6638-3c7b-4099-e331-cd4595b02744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Используемое устройство: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Используемое устройство: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-kdatAkBXyS6"
      },
      "outputs": [],
      "source": [
        "class PyTorchMLP(nn.Module):\n",
        "    \"\"\"MLP на PyTorch с 1 скрытым слоем.\"\"\"\n",
        "\n",
        "    def __init__(self, n_features, n_hidden=100, activation='sigmoid'):\n",
        "        super(PyTorchMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(n_features, n_hidden)\n",
        "        self.fc2 = nn.Linear(n_hidden, 1)\n",
        "\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = nn.Sigmoid()\n",
        "        elif activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Sigmoid()\n",
        "\n",
        "        self.output_activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.output_activation(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jsgpkWlhXyS6"
      },
      "outputs": [],
      "source": [
        "def train_pytorch_mlp(X_train, y_train, X_valid, y_valid,\n",
        "                      n_hidden=100, activation='sigmoid',\n",
        "                      learning_rate=0.01, n_epochs=100, batch_size=32,\n",
        "                      verbose=True):\n",
        "    \"\"\"Функция обучения PyTorch MLP с SGD.\"\"\"\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    # Подготовка данных для PyTorch\n",
        "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Создание модели\n",
        "    n_features = X_train.shape[1]\n",
        "    model = PyTorchMLP(n_features, n_hidden, activation).to(device)\n",
        "\n",
        "    # Loss и оптимизатор\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Обучение\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * len(y_batch)\n",
        "\n",
        "        if verbose and (epoch + 1) % 20 == 0:\n",
        "            print(f'Epoch {epoch+1}/{n_epochs} - Train Loss: {epoch_loss/len(X_train):.4f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZdaSdIZXyS7",
        "outputId": "7e8e66f8-f3a1-4ad4-d3d6-ef65fa85a10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обучение MLP на PyTorch\n",
            "Epoch 20/100 - Train Loss: 0.3227\n",
            "Epoch 40/100 - Train Loss: 0.2991\n",
            "Epoch 60/100 - Train Loss: 0.2941\n",
            "Epoch 80/100 - Train Loss: 0.2927\n",
            "Epoch 100/100 - Train Loss: 0.2917\n"
          ]
        }
      ],
      "source": [
        "print('Обучение MLP на PyTorch')\n",
        "\n",
        "pytorch_model = train_pytorch_mlp(\n",
        "    X_train, y_train, X_valid, y_valid,\n",
        "    n_hidden=100,\n",
        "    activation='sigmoid',\n",
        "    learning_rate=0.01,\n",
        "    n_epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcIpv4PaXyS7",
        "outputId": "9705a755-1af2-4995-96e3-8fb0a8fd74bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch MLP (SGD): Gini = 0.4893\n"
          ]
        }
      ],
      "source": [
        "# Оценка PyTorch модели\n",
        "pytorch_model.eval()\n",
        "with torch.no_grad():\n",
        "    X_valid_tensor = torch.FloatTensor(X_valid).to(device)\n",
        "    valid_proba_pytorch = pytorch_model(X_valid_tensor).cpu().numpy().flatten()\n",
        "\n",
        "pytorch_gini = compute_gini(y_valid, valid_proba_pytorch)\n",
        "print(f'PyTorch MLP (SGD): Gini = {pytorch_gini:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучше ли PyTorch чем моя реализация?\n",
        "\n",
        "Моя реализация лучше на 0.0013 Gini.\n",
        "\n",
        "Возможные причины небольшого расхождения в результатах:\n",
        "\n",
        "- Обе реализации используют одинаковый алгоритм (SGD без momentum), поэтому сравнение честное\n",
        "- В нашей NumPy-реализации мы вручную настроили Xavier инициализацию под sigmoid\n",
        "- NumPy считает в Float64 (высокая точность, 15 знаков после запятой), а PyTorch — в Float32 (7 знаков). За миллионы операций эта разница накапливается и даёт небольшое расхождение в результатах\n",
        "\n",
        "Итог сравнения:\n",
        "\n",
        "- My MLP (sigmoid): 0.4906\n",
        "- PyTorch MLP: 0.4893"
      ],
      "metadata": {
        "id": "08jxB_lqQ5RY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5BQpO4VXyS7"
      },
      "source": [
        "---\n",
        "## 7. Final Evaluation on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VguuuxOlXyS8",
        "outputId": "c2dced4c-7c58-4cb2-a04f-fbd8513a356a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Финальная оценка на тестовом датасете\n",
            "Лучшая модель: My MLP (sigmoid)\n",
            "\n",
            "Датасет            Gini\n",
            "------------------------\n",
            "Train            0.5448\n",
            "Valid            0.4906\n",
            "Test             0.4991\n"
          ]
        }
      ],
      "source": [
        "print('Финальная оценка на тестовом датасете')\n",
        "\n",
        "# Лучшая модель из сравнения активаций\n",
        "best_model = activation_results[best_activation]['model']\n",
        "best_model_name = f'My MLP ({best_activation})'\n",
        "\n",
        "# Предсказания на всех датасетах\n",
        "train_proba = best_model.predict_proba(X_train)[:, 1]\n",
        "valid_proba = best_model.predict_proba(X_valid)[:, 1]\n",
        "test_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Вычисление Gini\n",
        "train_gini = compute_gini(y_train, train_proba)\n",
        "valid_gini = compute_gini(y_valid, valid_proba)\n",
        "test_gini = compute_gini(y_test, test_proba)\n",
        "\n",
        "print(f'Лучшая модель: {best_model_name}')\n",
        "print(f'\\n{\"Датасет\":<12} {\"Gini\":>10}')\n",
        "print('-' * 24)\n",
        "print(f'{\"Train\":<12} {train_gini:>10.4f}')\n",
        "print(f'{\"Valid\":<12} {valid_gini:>10.4f}')\n",
        "print(f'{\"Test\":<12} {test_gini:>10.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Анализ разрывов (Gap Analysis)\n",
        "\n",
        "- Train - Valid gap: +0.0542\n",
        "- Valid - Test gap: -0.0085\n",
        "\n",
        "Наблюдается ли падение качества (Valid → Test)?\n",
        "\n",
        "Нет, Valid и Test Gini очень близки (gap = -0.0085).\n",
        "Модель хорошо обобщается на разные временные периоды.\n",
        "\n",
        "Переобучена ли модель (overfitting)?\n",
        "\n",
        "Ответ: Умеренное переобучение\n",
        "\n",
        "Объяснение:\n",
        "\n",
        "- Train Gini заметно выше Valid Gini.\n",
        "- Некоторое переобучение присутствует, но в допустимых пределах.\n",
        "- Модель выучивает полезные паттерны, но также немного шума.\n",
        "- Может помочь лёгкая регуляризация или меньше эпох."
      ],
      "metadata": {
        "id": "OvCPBFhWRopn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcDXhsRMXyS8"
      },
      "source": [
        "---\n",
        "## Bonus: Adam Optimizer Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Ehtf4NPIXyS8"
      },
      "outputs": [],
      "source": [
        "class MLPWithAdam(MLP):\n",
        "    \"\"\"\n",
        "    MLP с Adam оптимизатором и early stopping.\n",
        "    Наследует от базового MLP класса.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_hidden=100, activation='sigmoid', learning_rate=0.001,\n",
        "                 n_epochs=100, batch_size=32, random_state=42, verbose=True,\n",
        "                 beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
        "                 patience=15, min_delta=1e-4):\n",
        "        super().__init__(n_hidden, activation, learning_rate, n_epochs,\n",
        "                        batch_size, random_state, verbose)\n",
        "\n",
        "        # Adam гиперпараметры\n",
        "        self.beta1 = beta1    # Коэффициент для первого момента\n",
        "        self.beta2 = beta2    # Коэффициент для второго момента\n",
        "        self.epsilon = epsilon  # Защита от деления на ноль\n",
        "\n",
        "        # Early stopping параметры\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "\n",
        "        # Adam состояние (инициализируется при обучении)\n",
        "        self.m = {}  # Первый момент (скользящее среднее градиентов)\n",
        "        self.v = {}  # Второй момент (скользящее среднее квадратов градиентов)\n",
        "        self.t = 0   # Счётчик шагов\n",
        "\n",
        "        self.best_weights = None\n",
        "        self.best_valid_loss = float('inf')\n",
        "\n",
        "    def _initialize_adam_state(self):\n",
        "        \"\"\"Инициализация состояния Adam (нули для m и v).\"\"\"\n",
        "        self.m = {\n",
        "            'W1': np.zeros_like(self.W1), 'b1': np.zeros_like(self.b1),\n",
        "            'W2': np.zeros_like(self.W2), 'b2': np.zeros_like(self.b2)\n",
        "        }\n",
        "        self.v = {\n",
        "            'W1': np.zeros_like(self.W1), 'b1': np.zeros_like(self.b1),\n",
        "            'W2': np.zeros_like(self.W2), 'b2': np.zeros_like(self.b2)\n",
        "        }\n",
        "        self.t = 0\n",
        "\n",
        "    def _save_weights(self):\n",
        "        \"\"\"Сохранение текущих весов.\"\"\"\n",
        "        return {'W1': self.W1.copy(), 'b1': self.b1.copy(),\n",
        "                'W2': self.W2.copy(), 'b2': self.b2.copy()}\n",
        "\n",
        "    def _restore_weights(self, weights):\n",
        "        \"\"\"Восстановление весов.\"\"\"\n",
        "        self.W1 = weights['W1'].copy()\n",
        "        self.b1 = weights['b1'].copy()\n",
        "        self.W2 = weights['W2'].copy()\n",
        "        self.b2 = weights['b2'].copy()\n",
        "\n",
        "    def _update_weights_adam(self, gradients):\n",
        "        \"\"\"Обновление весов методом Adam.\"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        for param_name, grad_name in [('W1', 'dW1'), ('b1', 'db1'),\n",
        "                                       ('W2', 'dW2'), ('b2', 'db2')]:\n",
        "            g = gradients[grad_name]\n",
        "\n",
        "            # Обновление смещённой оценки первого момента\n",
        "            self.m[param_name] = self.beta1 * self.m[param_name] + (1 - self.beta1) * g\n",
        "\n",
        "            # Обновление смещённой оценки второго момента\n",
        "            self.v[param_name] = self.beta2 * self.v[param_name] + (1 - self.beta2) * (g ** 2)\n",
        "\n",
        "            # Коррекция смещения\n",
        "            m_corrected = self.m[param_name] / (1 - self.beta1 ** self.t)\n",
        "            v_corrected = self.v[param_name] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "            # Обновление параметров\n",
        "            param = getattr(self, param_name)\n",
        "            param -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n",
        "            setattr(self, param_name, param)\n",
        "\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None):\n",
        "        \"\"\"Обучение модели с Adam оптимизатором и early stopping.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        self._initialize_weights(n_features)\n",
        "        self._initialize_adam_state()\n",
        "\n",
        "        n_batches = int(np.ceil(n_samples / self.batch_size))\n",
        "        epochs_without_improvement = 0\n",
        "\n",
        "        for epoch in range(self.n_epochs):\n",
        "            indices = np.random.permutation(n_samples)\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "\n",
        "            for i in range(n_batches):\n",
        "                start_idx = i * self.batch_size\n",
        "                end_idx = min((i + 1) * self.batch_size, n_samples)\n",
        "\n",
        "                X_batch = X_shuffled[start_idx:end_idx]\n",
        "                y_batch = y_shuffled[start_idx:end_idx]\n",
        "\n",
        "                cache = self._forward(X_batch)\n",
        "                batch_loss = self._compute_loss(y_batch, cache['a2'])\n",
        "                epoch_loss += batch_loss * len(y_batch)\n",
        "\n",
        "                gradients = self._backward(y_batch, cache)\n",
        "                self._update_weights_adam(gradients)  # Adam вместо SGD\n",
        "\n",
        "            epoch_loss /= n_samples\n",
        "            self.train_losses.append(epoch_loss)\n",
        "\n",
        "            # Валидация и Early Stopping\n",
        "            if X_valid is not None:\n",
        "                valid_pred = self.predict_proba(X_valid)[:, 1]\n",
        "                valid_loss = self._compute_loss(y_valid, valid_pred)\n",
        "                self.valid_losses.append(valid_loss)\n",
        "\n",
        "                # Проверка улучшения\n",
        "                if valid_loss < self.best_valid_loss - self.min_delta:\n",
        "                    self.best_valid_loss = valid_loss\n",
        "                    self.best_weights = self._save_weights()\n",
        "                    epochs_without_improvement = 0\n",
        "                else:\n",
        "                    epochs_without_improvement += 1\n",
        "\n",
        "                # Early stopping\n",
        "                if epochs_without_improvement >= self.patience:\n",
        "                    if self.verbose:\n",
        "                        print(f\"Early stopping на эпохе {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "            if self.verbose and (epoch + 1) % 10 == 0:\n",
        "                msg = f\"Epoch {epoch+1}/{self.n_epochs} - Train Loss: {epoch_loss:.4f}\"\n",
        "                if X_valid is not None:\n",
        "                    msg += f\" - Valid Loss: {valid_loss:.4f}\"\n",
        "                print(msg)\n",
        "\n",
        "        # Восстановление лучших весов\n",
        "        if self.best_weights is not None:\n",
        "            self._restore_weights(self.best_weights)\n",
        "            if self.verbose:\n",
        "                print(f\"\\nВосстановлены лучшие веса (Valid Loss = {self.best_valid_loss:.4f})\")\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpr50fpNXyS9",
        "outputId": "4646fa5a-ae55-4dc6-cb48-10b0b1091f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BONUS: Реализация Adam оптимизатора\n",
            "Epoch 10/200 - Train Loss: 0.2760 - Valid Loss: 0.3618\n",
            "Epoch 20/200 - Train Loss: 0.2676 - Valid Loss: 0.4065\n",
            "Early stopping на эпохе 23\n",
            "\n",
            "Восстановлены лучшие веса (Valid Loss = 0.3380)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.MLPWithAdam at 0x7d74d60f0cb0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Обучение MLP с Adam оптимизатором\n",
        "print('BONUS: Реализация Adam оптимизатора')\n",
        "\n",
        "mlp_adam = MLPWithAdam(\n",
        "    n_hidden=100,\n",
        "    activation='sigmoid',\n",
        "    learning_rate=0.001,\n",
        "    n_epochs=200,\n",
        "    batch_size=32,\n",
        "    random_state=42,\n",
        "    verbose=True,\n",
        "    patience=20\n",
        ")\n",
        "\n",
        "mlp_adam.fit(X_train, y_train, X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXlrTp5fXyS-",
        "outputId": "9082f1c4-e750-42e8-a2a5-e1e077a26c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My MLP (Adam): Gini = 0.4796\n"
          ]
        }
      ],
      "source": [
        "# Оценка модели с Adam\n",
        "adam_proba = mlp_adam.predict_proba(X_valid)[:, 1]\n",
        "adam_gini = compute_gini(y_valid, adam_proba)\n",
        "\n",
        "print(f'My MLP (Adam): Gini = {adam_gini:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}